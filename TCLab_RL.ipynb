{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d814e8-2eea-4b68-a234-4f12157e9f43",
   "metadata": {},
   "source": [
    "#### TCLab with Reinforcement Learning\n",
    "\n",
    "This exercise demonstrates a Reinforcement Learning (RL) approach for controlling the Temperature Control Lab (TCLab) using a Deep Deterministic Policy Gradient (DDPG) algorithm in PyTorch. The RL agent learns to adjust the heater power to maintain a desired temperature setpoint.\n",
    "\n",
    "Reinforcement Learning (RL) is a machine learning method for optimal control where an **agent** learns to make decisions by interacting with an **environment** to maximize a cumulative **reward**. Unlike supervised learning, the agent isnâ€™t given correct actions but instead **experiments** with actions, learning through feedback (rewards). The agent observes the current **state** of the environment, takes an **action**, and receives a **reward** (scalar feedback signal), then the environment transitions to a new state. This loop continues over time. The agent seeks a **policy** (mapping states to actions) that maximizes expected cumulative rewards. Key concepts:\n",
    "\n",
    "*  **States**: Observations of the environment.\n",
    "*  **Actions**: Decisions made by the agent.\n",
    "*  **Rewards**: Immediate feedback signals.\n",
    "*  **Policy**: Strategy to select actions.\n",
    "\n",
    "Agents face the **exploration vs. exploitation dilemma**: exploring new actions to find higher rewards vs. exploiting known rewarding actions.\n",
    "\n",
    "**TCLab Environment**\n",
    "\n",
    "The [TCLab](https://apmonitor.com/heat.htm) is an Arduino-based temperature control system with:\n",
    "\n",
    "* Two heaters\n",
    "* Two temperature sensors\n",
    "* Python / MATLAB / Simulink interface\n",
    "\n",
    "The RL agent learns to control heater power to maintain a temperature set point.\n",
    "\n",
    "**Gymnasium Custom Environment**\n",
    "\n",
    "Define a custom Gymnasium environment to interface with TCLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fea3a7-ab6b-4569-8871-94631754242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e8f92-6037-4bcf-8e1e-9619474ea03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tclab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bda68-5934-4aab-9c1c-f95cec09d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import tclab\n",
    "\n",
    "class TCLabEnv(gym.Env):\n",
    "    def __init__(self, setpoint=50):\n",
    "        super(TCLabEnv, self).__init__()\n",
    "        self.lab = tclab.TCLabModel()  # Connect to TCLab hardware with TCLab()\n",
    "        self.setpoint = setpoint\n",
    "        self.action_space = gym.spaces.Box(low=np.array([0]), high=np.array([100]), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0]), high=np.array([100]), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.lab.Q1(0)  # Turn off heater\n",
    "        self.lab.Q2(0)\n",
    "        return np.array([self.lab.T1]), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.lab.Q1(action[0])  # Apply action\n",
    "        self.lab.Q2(action[0])\n",
    "        temperature = self.lab.T1  # Read temperature\n",
    "        reward = -abs(temperature - self.setpoint)  # Reward: minimize error\n",
    "        done = False  # No terminal state in continuous control\n",
    "        return np.array([temperature]), reward, done, False, {}\n",
    "\n",
    "    def close(self):\n",
    "        self.lab.Q1(0)\n",
    "        self.lab.Q2(0)\n",
    "        self.lab.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ca94-cdf0-49bf-b9c5-1be3597391d4",
   "metadata": {},
   "source": [
    "#### Deep Deterministic Policy Gradient (DDPG)\r\n",
    "\r\n",
    "**Overview:** DDPG combines policy gradients and Q-learning for continuous action spaces using actor-critic methods.\r\n",
    "\r\n",
    "**Updates:**\r\n",
    "*  **Critic Loss:**\r\n",
    "$ L(\\theta_Q) = \\left(Q_{\\theta_Q}(s,a) - [r + \\gamma Q_{\\theta_Q^-}(s',\\mu_{\\theta_\\mu^-}(s'))]\\right)^2 $\r\n",
    "*  **Actor Update:**\r\n",
    "$ \\nabla_{\\theta_\\mu} J \\approx \\mathbb{E}_{s}\\left[\\nabla_a Q_{\\theta_Q}(s,a)|_{a=\\mu(s)}\\nabla_{\\theta_\\mu}\\mu_{\\theta_\\mu}(s)\\right] $\r\n",
    "\r\n",
    "**Applications:** Robotics (pendulum balancing), chemical process control, portfolio management, voltage regulation in power systems.\r\n",
    "\r\n",
    "###  Definition of Symbols\r\n",
    "\r\n",
    "*  $Q(s,a)$: Action-value function.\r\n",
    "*  $\\alpha$: Learning rate.\r\n",
    "*  $\\gamma$: Discount factor.\r\n",
    "*  $r$: Reward.\r\n",
    "*  $\\theta$: Parameters of neural network or policy.\r\n",
    "*  $\\pi_{\\theta}(a|s)$: Policy distribution parameterized by $\\theta$.\r\n",
    "*  $G_t$: Return (cumulative discounted reward from time $t$ onwards).\r\n",
    "*  $\\hat{A}_t$: Advantage estimate at time $t$.\r\n",
    "*  $\\epsilon$: PPO clip range hyperparameter.\r\n",
    "*  $\\mu(s)$: Deterministic policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16a4cc-b4f7-499f-b048-278a4e647c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        if action.dim() == 1:  # If action is 1D, reshape it to (batch_size, action_dim)\n",
    "            action = action.unsqueeze(1)\n",
    "        return self.net(torch.cat([state, action], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cccd3cf-1d30-4dfb-8c52-9df2b33ef814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32), \n",
    "            torch.tensor(actions, dtype=torch.float32),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):  # Add this method\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3146df-0619-4b9b-9675-b66ec4f1fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gymnasium environment\n",
    "env = TCLabEnv(setpoint=50)\n",
    "\n",
    "actor = Actor(state_dim=1, action_dim=1, max_action=100)\n",
    "critic = Critic(state_dim=1, action_dim=1)\n",
    "target_actor = Actor(state_dim=1, action_dim=1, max_action=100)\n",
    "target_critic = Critic(state_dim=1, action_dim=1)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "buffer = ReplayBuffer()\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(200):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = actor(state_tensor).detach().cpu().numpy().flatten()[0]  # Convert to scalar\n",
    "        next_state, reward, done, _, _ = env.step([action])  # Wrap action in list\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(buffer) > 64:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(64)\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states)\n",
    "                target_q = target_critic(next_states, next_actions)\n",
    "                target_q = rewards + gamma * (1 - dones) * target_q\n",
    "            current_q = critic(states, actions)\n",
    "            critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            actor_loss = -critic(states, actor(states)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
