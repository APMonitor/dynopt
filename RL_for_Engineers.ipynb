{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d814e8-2eea-4b68-a234-4f12157e9f43",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning for Engineers\n",
    "\n",
    "Reinforcement Learning (RL) is a paradigm of machine learning and optimal control where an **agent** learns to make decisions by interacting with an **environment** to maximize a cumulative **reward**. Unlike supervised learning, the agent isnâ€™t given correct actions but instead **experiments** with actions, learning through feedback (rewards). The agent observes the current **state** of the environment, takes an **action**, and receives a **reward** (scalar feedback signal), then the environment transitions to a new state. This loop continues over time (see **Figure 1**). The agent seeks a **policy** (mapping states to actions) that maximizes expected cumulative rewards. Key concepts:\n",
    "\n",
    "*  **States**: Observations of the environment.\n",
    "*  **Actions**: Decisions made by the agent.\n",
    "*  **Rewards**: Immediate feedback signals.\n",
    "*  **Policy**: Strategy to select actions.\n",
    "\n",
    "Agents face the **exploration vs. exploitation dilemma**: exploring new actions to find higher rewards vs. exploiting known rewarding actions.\n",
    "\n",
    "**Markov Decision Processes (MDPs):**\n",
    "\n",
    "RL problems often use Markov Decision Processes (MDPs), a mathematical framework for sequential decision-making under uncertainty. An MDP is defined by a tuple:\n",
    "\n",
    "$ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma) $\n",
    "\n",
    "where:\n",
    "\n",
    "*  $\\mathcal{S}$ = Set of **states**.\n",
    "*  $\\mathcal{A}$ = Set of **actions**.\n",
    "*  $P(s'|s,a)$ = **Transition probability** to state $s'$ given state $s$ and action $a$.\n",
    "*  $R(s,a,s')$ = **Reward** from transitioning state $s$ to $s'$ via action $a$.\n",
    "*  $\\gamma \\in [0,1)$ = **Discount factor**, weighing future vs. immediate rewards.\n",
    "\n",
    "MDPs satisfy the **Markov property**: future states depend only on current state and action. The **optimal policy** $\\pi^*(s)$ maximizes expected long-term rewards. MDP solutions involve computing **value functions** $V(s)$ or **action-value functions** $Q(s,a)$. The **Bellman optimality equation** for value functions:\n",
    "\n",
    "$ V^*(s) = \\max_{a \\in \\mathcal{A}} \\sum_{s'} P(s'|s,a)\\left[ R(s,a,s') + \\gamma V^*(s') \\right] $\n",
    "\n",
    "Similarly, the optimal Q-value:\n",
    "\n",
    "$ Q^*(s,a) = \\sum_{s'}P(s'|s,a)\\left[R(s,a,s') + \\gamma \\max_{a'}Q^*(s',a')\\right] $\n",
    "\n",
    "**Model-Free vs. Model-Based RL:**\n",
    "\n",
    "A key distinction:\n",
    "\n",
    "*  **Model-based RL**: Uses or learns a model $P(s'|s,a)$ and reward function, enabling planning and simulation (e.g., AlphaZero).\n",
    "*  **Model-free RL**: Learns directly from trial-and-error interaction, without explicit models. Common and simpler, but typically requires more environment interactions.\n",
    "\n",
    "Hybrid approaches like Dyna-Q use learned models to simulate additional experiences.\n",
    "\n",
    "**Applications of RL in Engineering Optimization:**\n",
    "\n",
    "RL applies broadly to engineering tasks involving sequential decision-making or control:\n",
    "\n",
    "*  **Chemical Engineering**: Process control and reaction optimization (e.g., reactor settings, energy minimization, yield improvement).\n",
    "*  **Mechanical Engineering**: Robotic control, autonomous systems (e.g., robotic arms, drones, inverted pendulum).\n",
    "*  **Automotive**: Autonomous driving (lane-keeping, cruise control, collision avoidance).\n",
    "*  **Industrial Energy Management**: HVAC optimization (e.g., DeepMind reduced Google data center cooling energy by ~40%).\n",
    "\n",
    "These applications demonstrate RLâ€™s effectiveness in engineering optimization, addressing complex, uncertain, and dynamic conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ca94-cdf0-49bf-b9c5-1be3597391d4",
   "metadata": {},
   "source": [
    "#### Deep Deterministic Policy Gradient (DDPG)\r\n",
    "\r\n",
    "**Overview:** DDPG combines policy gradients and Q-learning for continuous action spaces using actor-critic methods.\r\n",
    "\r\n",
    "**Updates:**\r\n",
    "*  **Critic Loss:**\r\n",
    "$ L(\\theta_Q) = \\left(Q_{\\theta_Q}(s,a) - [r + \\gamma Q_{\\theta_Q^-}(s',\\mu_{\\theta_\\mu^-}(s'))]\\right)^2 $\r\n",
    "*  **Actor Update:**\r\n",
    "$ \\nabla_{\\theta_\\mu} J \\approx \\mathbb{E}_{s}\\left[\\nabla_a Q_{\\theta_Q}(s,a)|_{a=\\mu(s)}\\nabla_{\\theta_\\mu}\\mu_{\\theta_\\mu}(s)\\right] $\r\n",
    "\r\n",
    "**Applications:** Robotics (pendulum balancing), chemical process control, portfolio management, voltage regulation in power systems.\r\n",
    "\r\n",
    "###  Definition of Symbols\r\n",
    "\r\n",
    "*  $Q(s,a)$: Action-value function.\r\n",
    "*  $\\alpha$: Learning rate.\r\n",
    "*  $\\gamma$: Discount factor.\r\n",
    "*  $r$: Reward.\r\n",
    "*  $\\theta$: Parameters of neural network or policy.\r\n",
    "*  $\\pi_{\\theta}(a|s)$: Policy distribution parameterized by $\\theta$.\r\n",
    "*  $G_t$: Return (cumulative discounted reward from time $t$ onwards).\r\n",
    "*  $\\hat{A}_t$: Advantage estimate at time $t$.\r\n",
    "*  $\\epsilon$: PPO clip range hyperparameter.\r\n",
    "*  $\\mu(s)$: Deterministic policy function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b413826-f5b7-4b10-8a3e-c31a7961fb9b",
   "metadata": {},
   "source": [
    "#### Hand Tracking with MediaPipe\n",
    "\n",
    "Use MediaPipe Hands to track hand positions and control a cart-pole system and an LED. Hand gestures are detected in real-time using a webcam and used to interact with simulated or physical objects. Use computer vision for hand detection with MediaPipe and control a cart-pole balancing system using hand movements. Adjust LED brightness and blinking rate with hand tracking and explore applications in machine learning, automation, and control systems.\n",
    "\n",
    "ðŸ“„ <a href='https://apmonitor.com/pds/index.php/Main/HandTracking'>Hand Tracking Activity</a>\n",
    "\n",
    "Below is code to run gymnasium CartPole-v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fea3a7-ab6b-4569-8871-94631754242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cccd3cf-1d30-4dfb-8c52-9df2b33ef814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3146df-0619-4b9b-9675-b66ec4f1fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "observation = env.reset()\n",
    "for _ in range(10):\n",
    "  env.render()\n",
    "  # Input:\n",
    "  #   Force to the cart with actions: 0=left, 1=right\n",
    "  # Returns:\n",
    "  #   obs = cart position, cart velocity, pole angle, rot rate\n",
    "  #   reward = +1 for every timestep\n",
    "  #   done = True when abs(angle)>15 or abs(cart pos)>2.4\n",
    "  action = env.action_space.sample() # random action\n",
    "  observation, reward, done, info, e = env.step(action)\n",
    "\n",
    "  print(observation)\n",
    "\n",
    "  if done:\n",
    "    observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce638d-f21e-4a52-900d-8854911c127c",
   "metadata": {},
   "source": [
    "#### Pendulum RL with DDPG\n",
    "\n",
    "See additional explanation at <a href='https://apmonitor.com/do/index.php/Main/RLGymnasium'>RL with Gymnasium</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251881d-ca3f-4216-9124-ef6b0a129f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "\n",
    "# Make Pendulum environment\n",
    "env = gym.make('Pendulum-v1', g=9.81)  # g is gravitational acceleration, default 10\n",
    "state_dim = env.observation_space.shape[0]   # dimension of state (should be 3 for Pendulum)\n",
    "action_dim = env.action_space.shape[0]       # dimension of action (1 for Pendulum)\n",
    "max_action = float(env.action_space.high[0]) # max torque (=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743ed6c-4139-4000-8703-902a69122363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network: maps state -> action (within [-max_action, max_action])\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        # Simple 2-layer MLP\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    def forward(self, state):\n",
    "        # Output raw action, then scale to range [-max_action, max_action] using tanh\n",
    "        raw_action = self.net(state)\n",
    "        # bound output action between -1 and 1 via tanh, then scale\n",
    "        action = self.max_action * torch.tanh(raw_action)\n",
    "        return action\n",
    "\n",
    "# Critic Network: maps (state, action) -> Q-value\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Q-network takes state and action concatenated\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, state, action):\n",
    "        # Ensure state and action are concatenated as vectors\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(1)\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        Q = self.net(x)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b9221-6560-4d18-963f-28edededdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pos = 0  # position to insert next entry (for circular buffer)\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        # Move position pointer (overwrite oldest if full)\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*(self.buffer[i] for i in batch))\n",
    "        # Convert to torch tensors\n",
    "        return (torch.tensor(np.array(states), dtype=torch.float32),\n",
    "                torch.tensor(np.array(actions), dtype=torch.float32),\n",
    "                torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(1),\n",
    "                torch.tensor(np.array(next_states), dtype=torch.float32),\n",
    "                torch.tensor(np.array(dones), dtype=torch.float32).unsqueeze(1))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f3d79-a7de-4be6-be99-b3fe6a0c98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize actor, critic, target networks and optimizers\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "target_actor = Actor(state_dim, action_dim, max_action)\n",
    "target_critic = Critic(state_dim, action_dim)\n",
    "# Copy weights from actor to target_actor, and critic to target_critic\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "target_actor.eval()\n",
    "target_critic.eval()\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "buffer = ReplayBuffer(capacity=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f9f98-51ae-42d0-ae9b-ed45e528ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "num_episodes = 200       # number of episodes to train\n",
    "batch_size = 64          # batch size for sampling from replay\n",
    "gamma = 0.99             # discount factor\n",
    "tau = 0.005              # target network update rate (tau)\n",
    "exploration_noise = 0.1  # stddev for Gaussian exploration noise\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = state.astype(np.float32)\n",
    "    episode_reward = 0.0\n",
    "    for step in range(500):  # max steps per episode (Pendulum typically truncated at 200)\n",
    "        # Select action according to current policy + exploration noise\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = actor(state_tensor).cpu().numpy()[0]\n",
    "        # Add exploration noise (Gaussian)\n",
    "        action = action + np.random.normal(0, exploration_noise * max_action, size=action_dim)\n",
    "        action = np.clip(action, -max_action, max_action)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        # Store transition in replay buffer\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Train the networks if we have enough samples in replay buffer\n",
    "        if len(buffer) >= batch_size:\n",
    "            # Sample a batch\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            # Compute target Q values using target networks\n",
    "            with torch.no_grad():\n",
    "                # Target actor for next action\n",
    "                next_actions = target_actor(next_states)\n",
    "                target_Q = target_critic(next_states, next_actions)\n",
    "                # If done (terminal), no future reward; use (1-done) mask\n",
    "                target_Q = rewards + gamma * (1 - dones) * target_Q\n",
    "            # Critic loss = MSE between current Q and target Q\n",
    "            current_Q = critic(states, actions)\n",
    "            critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "            # Update critic\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Actor loss = -mean(Q) (because we want to maximize Q, so minimize -Q)\n",
    "            actor_actions = actor(states)\n",
    "            actor_loss = -critic(states, actor_actions).mean()\n",
    "            # Update actor\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Soft update target networks\n",
    "            for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        if done:\n",
    "            break  # episode ends\n",
    "    # Logging (print) the cumulative reward of the episode\n",
    "    print(f\"Episode {episode+1}: Reward = {episode_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99598bf-171a-4346-b07e-b6060528a069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
